train_micro_batch_size_per_gpu: 8
gradient_accumulation_steps: 1
optimizer:
  type: AdamW
  params:
    lr: 3.0e-04
    betas:
      - 0.9
      - 0.95
    eps: 1.0e-08
    weight_decay: 0.1
fp16:
  enabled: true
  autocast: true
gradient_clipping: 1.0
zero_optimization:
  stage: 2
  contiguous_gradients: true
  overlap_comm: true
steps_per_print: 1.0e+10
mcpt:
  warmup_tokens: 2.0e+5
  decay_tokens: 4.0e+5